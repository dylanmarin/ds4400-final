{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump, load\n",
    "import pandas as pd\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import ReLU\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Embedding, LSTM, Dense, Input, add\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from common import RANDOM_SEED, clean_descriptions, ALL_FILENAMES, TRAIN_FILENAMES, VALIDATION_FILENAMES, TEST_FILENAMES, TRAIN_AND_VAL_FILENAMES, samples_to_dict, import_image_features, max_and_average_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image                              1000268201_693b08cb0e.jpg\n",
      "caption    [<start>, a, child, in, a, pink, dress, is, cl...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# import the cleaned data and print one example\n",
    "cleaned_data = clean_descriptions('data/flickr_8k/captions.txt')\n",
    "print(cleaned_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "get the actual rows from the df corresponding to the different sets\n",
    "'''\n",
    "# get the samples with the given filenames\n",
    "train_samples = cleaned_data.loc[cleaned_data['image'].isin(TRAIN_FILENAMES)]\n",
    "validation_samples = cleaned_data.loc[cleaned_data['image'].isin(VALIDATION_FILENAMES)]\n",
    "test_samples = cleaned_data.loc[cleaned_data['image'].isin(TEST_FILENAMES)]\n",
    "train_and_val_samples = cleaned_data.loc[cleaned_data['image'].isin(TRAIN_AND_VAL_FILENAMES)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image                              1000268201_693b08cb0e.jpg\n",
       "caption    [<start>, a, child, in, a, pink, dress, is, cl...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_dict = samples_to_dict(train_samples)\n",
    "validation_dict = samples_to_dict(validation_samples)\n",
    "train_and_val_dict = samples_to_dict(train_and_val_samples)\n",
    "test_dict = samples_to_dict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_features = import_image_features('8k_features.pkl', ALL_FILENAMES)\n",
    "\n",
    "train_image_features = import_image_features('8k_features.pkl', TRAIN_FILENAMES)\n",
    "val_image_features = import_image_features('8k_features.pkl', VALIDATION_FILENAMES)\n",
    "train_and_val_image_features = import_image_features('8k_features.pkl', TRAIN_AND_VAL_FILENAMES)\n",
    "test_image_features = import_image_features('8k_features.pkl', TEST_FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[37, 35, 35, 35, 34, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30]\n",
      "The longest sequence length from the training and validation samples is 37\n",
      "The average sequence length from the training and validation samples is 12\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH, AVG_LENGTH = max_and_average_sequence_length(train_and_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_of_specific_size(image_vector, descriptions, desired_caption_size, tokenizer):\n",
    "    '''\n",
    "    Given:\n",
    "    - one image vector representing a single image\n",
    "    - one value from our samples dict for the corresponding image (a list of 5 tokenized captions)\n",
    "    - the desired caption size N\n",
    "    - a Tokenizer\n",
    "\n",
    "    Return X:\n",
    "        a tensor with 5 elements (one for each caption) where each item is an array with length 4096 + N\n",
    "        - the first 4096 elements are the VGG extracted features of the corresponding image\n",
    "        - the next N elements are the first N words of the caption (converted to numbers by the passed in tokenizer)\n",
    "    And Y:\n",
    "        a tensor with 5 elements where each element is:\n",
    "        - the N+1 word in the sequence\n",
    "\n",
    "    NOTE: If any caption has a total length less than or equal to N, then it will not be added to the output, \n",
    "    meaning that the output could be tensors with 0 dimensions\n",
    "    '''\n",
    "    # initalize empty arrays for the samples\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # reshape the image vector to be one dimension\n",
    "    image_vector = image_vector.reshape(-1,)\n",
    "                                \n",
    "    # convert the descriptions to number lists instead of string lists\n",
    "    descriptions = tokenizer.texts_to_sequences(descriptions)\n",
    "\n",
    "    # for each description\n",
    "    for description in descriptions:\n",
    "        # only if the caption is at least 1 longer than N (desired_caption_size)\n",
    "        if len(description) + 1 >= desired_caption_size:\n",
    "            # get the caption as a numpy array of elements (length will be desired caption size)\n",
    "            caption = np.array(description[:desired_caption_size])\n",
    "        \n",
    "            # concatenate the image vector with the caption vector\n",
    "            combined_X = np.concatenate([image_vector, caption])\n",
    "\n",
    "            # append the combined X vector to the output x list\n",
    "            X.append(combined_X)\n",
    "\n",
    "            # get the one-hot encoding of the last word (required for keras models)\n",
    "            # and append it to the output y list\n",
    "            last_word = description[desired_caption_size]\n",
    "            y.append(to_categorical(last_word, VOCAB_SIZE)[0])\n",
    "\n",
    "    return tf.convert_to_tensor(X), tf.convert_to_tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(filename_description_dictionary, desired_caption_size, loops, tokenizer):\n",
    "    '''\n",
    "    Given:\n",
    "    - A dictionary containing the samples we want to create a generator for where\n",
    "        key: filename (string)\n",
    "        value: list of captions (where each caption is a list of strings)\n",
    "    - A desired caption size (N)\n",
    "    - A number of loops L\n",
    "    - A tokenizer for converting seen words to numbers\n",
    "    \n",
    "    loops are used because a generator is expended once it yields its last result, and \n",
    "    therefore cannot be used over multiple epohchs\n",
    "\n",
    "    Iterate through the filename_description dictionary (L times).\n",
    "    For each filename, generate corresponding number of samples where the caption size is N\n",
    "    Each value in the X samples vector will be 4096 + N\n",
    "        Those N values are the first N words of the corresponding caption\n",
    "    Each value in the y vector will be the N+1 word\n",
    "\n",
    "    Used to save memory\n",
    "    Each loop it shuffles the order of the samples\n",
    "    '''\n",
    "    for _ in range(loops):\n",
    "        # shuffle filename order for better distribution over multiple loops (epochs)\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        all_filenames = list(filename_description_dictionary.keys())\n",
    "        np.random.shuffle(all_filenames)\n",
    "        \n",
    "        # loop for ever over filenames\n",
    "        for filename in all_filenames:\n",
    "            # get the corresponding descriptions\n",
    "            descriptions = filename_description_dictionary[filename]\n",
    "\n",
    "            # retrieve the image feature vector\n",
    "            image_vector = all_image_features[filename][0]\n",
    "\n",
    "            # get the samples of the desired size (N) \n",
    "            x_samples, y_samples = get_samples_of_specific_size(image_vector, descriptions, desired_caption_size, tokenizer)\n",
    "            yield x_samples, y_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REMOVE IF NOT USED\n",
    "\n",
    "# def dictionary_to_model_samples(dictionary, image_features, max_length):\n",
    "#     # list of image features concatenated with corresponding first i-1 words\n",
    "#     all_Xs = [[] for i in range(max_length)]\n",
    "\n",
    "#     # next word for corresponding sentence\n",
    "#     all_ys = [[] for i in range(max_length)]\n",
    "\n",
    "#     for filename, samples in dictionary.items():\n",
    "#         samples = tokenizer.texts_to_sequences(samples)\n",
    "#         for sample in samples:\n",
    "#             for i in range(len(sample) - 1):\n",
    "\n",
    "#                 if i > max_length - 1:\n",
    "#                     break \n",
    "\n",
    "#                 x1 = image_features[filename].reshape(-1,)\n",
    "#                 x2 = np.array(sample[:i + 1])\n",
    "\n",
    "#                 combined_X = np.concatenate([x1, x2])\n",
    "\n",
    "#                 y = to_categorical(sample[i+1], VOCAB_SIZE)\n",
    "            \n",
    "#                 all_Xs[i].append(combined_X)\n",
    "#                 all_ys[i].append(y)\n",
    "\n",
    "\n",
    "#     all_Xs = [tf.convert_to_tensor(samples) for samples in all_Xs]\n",
    "#     all_ys = [tf.convert_to_tensor(samples) for samples in all_ys]\n",
    "    \n",
    "#     return all_Xs, all_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logistic_model(input_size):\n",
    "    '''\n",
    "    generate a logistic regression model using keras api\n",
    "    \n",
    "    since our model uses multiple logistic regression models, \n",
    "    we wanted to run it on the gpu which is simple with keras\n",
    "    '''\n",
    "    # create a linear activation function, relu which doesn't punish values < 0\n",
    "    linear_activation = ReLU(negative_slope=1)\n",
    "\n",
    "    # FF NN\n",
    "    model = Sequential()\n",
    "\n",
    "    # input layer is given input size (4096 + number of words for corresponding decoder)\n",
    "    model.add(Dense(input_size, activation=linear_activation))\n",
    "\n",
    "    # output layer with softmax for the whole vocabulary\n",
    "    model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "\n",
    "    # compile and return\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8080\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_samples['caption']))\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ALL_FILENAMES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10268\\557758774.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mALL_FILENAMES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ALL_FILENAMES' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test1, test2 = train_test_split(ALL_FILENAMES, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticDecoder():\n",
    "    '''\n",
    "    this class represents a model that can generate captions for text based on image input\n",
    "\n",
    "    it works by creating a logistic regression classifier for each position in the output string\n",
    "\n",
    "    each logistic regression model is assigned an index corresponding to the number of words it takes as input\n",
    "    for example:\n",
    "        model at index 3 is responsible for taking in a feature vector of length 4096 + 3\n",
    "        the first 4096 values are the image input, and the +3 represents the first 3 strings in the caption\n",
    "        the model predicts the 4th word\n",
    "    '''\n",
    "\n",
    "    def __init__(self, caption_max_length):\n",
    "        '''\n",
    "        given a max caption length N, initalize N logistic regression models, one for each position in our caption length\n",
    "        '''\n",
    "        self.max_len = caption_max_length\n",
    "\n",
    "        # generate a model that takes in an image feature vector, and the caption so far, and outputs the next word\n",
    "        self.models = [generate_logistic_model(4096 + i + 1) for i in range(caption_max_length)]\n",
    "\n",
    "\n",
    "    def fit(self, sample_dictionary, epochs, verbose=False):\n",
    "        for i in range(self.max_len):\n",
    "            if verbose:\n",
    "                print(f'Training model #{i+1}')\n",
    "\n",
    "            # TODO deal with tokenizer\n",
    "            # TODO deal with saving \n",
    "            # TODO deal with importing\n",
    "            # TODO do prediction\n",
    "\n",
    "            \n",
    "            current_generator = data_generator(sample_dictionary, desired_caption_size=i+1, loops=epochs, )\n",
    "            self.models[i].fit(current_generator)\n",
    "\n",
    "    def prediction(self, image_filename):\n",
    "        for i in range(self.max_len):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_decoder = LogisticDecoder(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = logistic_decoder.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_generator = data_generator(validation_dict, val_image_features, 1, loops=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toyso\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 125s 154ms/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c211bd0ec8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.fit_generator(first_generator, epochs=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0184f448d6494873b5885b7cafa76c11f0e318a0940098d9222ca8536d4b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

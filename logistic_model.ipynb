{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump, load\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from keras.layers import Dropout, Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.layers import Dropout, Embedding, LSTM, Dense, Input, add\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "# import keras\n",
    "# keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = pd.read_csv('data/flickr_8k/captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_description(text):\n",
    "    '''\n",
    "    returns new array of tokens representing the text\n",
    "\n",
    "    - lowercased\n",
    "    - removes 1 - letter punctuation\n",
    "    - removes numbers\n",
    "    - appends 's to previous words\n",
    "    - reconstructs string\n",
    "\n",
    "    <start> is appended to the start\n",
    "    <end> is appended to the end\n",
    "\n",
    "    Notes:\n",
    "    maybe keep in numbers\n",
    "    maybe remove all dashes \n",
    "    '''\n",
    "    output = []\n",
    "\n",
    "    text = text.lower().replace('\"', '')\n",
    "    \n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        if token.isalpha() or ((not token.isalpha() and len(token) > 1) and not token.isnumeric()):\n",
    "            output.append(token)\n",
    "\n",
    "    for i, token in enumerate(output):\n",
    "        if token == \"'s\":\n",
    "            output[i-1] = output[i-1] + \"'s\"\n",
    "            output.remove(\"'s\")\n",
    "\n",
    "        if len(token) == 2 and '.' in token:\n",
    "            output[i] = token.replace('.', '')\n",
    "    \n",
    "    output = ['<start>'] + output + ['<end>']\n",
    "\n",
    "    return output\n",
    "\n",
    "def clean_descriptions(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    data['caption'] = data['caption'].apply(lambda caption: clean_description(caption))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           image  \\\n",
      "0      1000268201_693b08cb0e.jpg   \n",
      "1      1000268201_693b08cb0e.jpg   \n",
      "2      1000268201_693b08cb0e.jpg   \n",
      "3      1000268201_693b08cb0e.jpg   \n",
      "4      1000268201_693b08cb0e.jpg   \n",
      "...                          ...   \n",
      "40450   997722733_0cb5439472.jpg   \n",
      "40451   997722733_0cb5439472.jpg   \n",
      "40452   997722733_0cb5439472.jpg   \n",
      "40453   997722733_0cb5439472.jpg   \n",
      "40454   997722733_0cb5439472.jpg   \n",
      "\n",
      "                                                 caption  \n",
      "0      [<start>, a, child, in, a, pink, dress, is, cl...  \n",
      "1      [<start>, a, girl, going, into, a, wooden, bui...  \n",
      "2      [<start>, a, little, girl, climbing, into, a, ...  \n",
      "3      [<start>, a, little, girl, climbing, the, stai...  \n",
      "4      [<start>, a, little, girl, in, a, pink, dress,...  \n",
      "...                                                  ...  \n",
      "40450  [<start>, a, man, in, a, pink, shirt, climbs, ...  \n",
      "40451  [<start>, a, man, is, rock, climbing, high, in...  \n",
      "40452  [<start>, a, person, in, a, red, shirt, climbi...  \n",
      "40453  [<start>, a, rock, climber, in, a, red, shirt,...  \n",
      "40454  [<start>, a, rock, climber, practices, on, a, ...  \n",
      "\n",
      "[40455 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = clean_descriptions('data/flickr_8k/captions.txt')\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = list(set(cleaned_data['image']))\n",
    "train_filenames, test_filenames = train_test_split(all_filenames, test_size=0.2, random_state=RANDOM_SEED)\n",
    "test_filenames, validation_filenames = train_test_split(test_filenames, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "training_samples = cleaned_data.loc[cleaned_data['image'].isin(train_filenames)]\n",
    "validation_samples = cleaned_data.loc[cleaned_data['image'].isin(validation_filenames)]\n",
    "test_samples = cleaned_data.loc[cleaned_data['image'].isin(test_filenames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, child, in, a, pink, dress, is, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, girl, going, into, a, wooden, bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, little, girl, climbing, into, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, little, girl, climbing, the, stai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, little, girl, in, a, pink, dress,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, man, in, a, pink, shirt, climbs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, man, is, rock, climbing, high, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, person, in, a, red, shirt, climbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, rock, climber, in, a, red, shirt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>[&lt;start&gt;, a, rock, climber, practices, on, a, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32360 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      [<start>, a, child, in, a, pink, dress, is, cl...  \n",
       "1      [<start>, a, girl, going, into, a, wooden, bui...  \n",
       "2      [<start>, a, little, girl, climbing, into, a, ...  \n",
       "3      [<start>, a, little, girl, climbing, the, stai...  \n",
       "4      [<start>, a, little, girl, in, a, pink, dress,...  \n",
       "...                                                  ...  \n",
       "40450  [<start>, a, man, in, a, pink, shirt, climbs, ...  \n",
       "40451  [<start>, a, man, is, rock, climbing, high, in...  \n",
       "40452  [<start>, a, person, in, a, red, shirt, climbi...  \n",
       "40453  [<start>, a, rock, climber, in, a, red, shirt,...  \n",
       "40454  [<start>, a, rock, climber, practices, on, a, ...  \n",
       "\n",
       "[32360 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8163\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(training_samples['caption']))\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dict(samples):\n",
    "\t\n",
    "\t# TODO comment\n",
    "\n",
    "\tdescriptions = dict()\n",
    "\tfor image, caption in zip(samples['image'], samples['caption']):\n",
    "\t\tif image not in descriptions.keys():\n",
    "\t\t\tdescriptions[image] = [caption]\n",
    "\t\telse:\n",
    "\t\t\tdescriptions[image].append(caption)\t\n",
    "\n",
    "\t\t\n",
    "\treturn descriptions\n",
    "\n",
    "training_dict = samples_to_dict(training_samples)\n",
    "validation_dict = samples_to_dict(validation_samples)\n",
    "test_dict = samples_to_dict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_image_features(features_file, corresponding_filenames):\n",
    "    '''\n",
    "    from our stored pkl file of extracted VGG features,\n",
    "    given the pkl file name, and a list of photo filenames\n",
    "    return a dictionary from filename to VGG features\n",
    "    '''\n",
    "    # import all features from pkl file\n",
    "    all_features = load(open(features_file, 'rb'))\n",
    "    \n",
    "    # get a dictionary from filename to image features\n",
    "    # splits filename at '.' because the pkl file doesnt store the .jpg part of the filename\n",
    "    features = {filename: all_features[filename.split('.')[0]] for filename in corresponding_filenames}\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_features = import_image_features('8k_features.pkl', train_filenames)\n",
    "test_image_features = import_image_features('8k_features.pkl', test_filenames)\n",
    "val_image_features = import_image_features('8k_features.pkl', validation_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(training_samples['caption'].apply(lambda caption : len(caption)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try W2V embeddings later if model sucks\n",
    "\n",
    "def dictionary_to_model_samples(dictionary, image_features, max_length):\n",
    "    # list of image features concatenated with corresponding first i-1 words\n",
    "    all_Xs = [[] for i in range(max_length)]\n",
    "\n",
    "    # next word for corresponding sentence\n",
    "    all_ys = [[] for i in range(max_length)]\n",
    "\n",
    "    for filename, samples in dictionary.items():\n",
    "        samples = tokenizer.texts_to_sequences(samples)\n",
    "        for sample in samples:\n",
    "            for i in range(len(sample) - 1):\n",
    "\n",
    "                if i > max_length - 1:\n",
    "                    break \n",
    "\n",
    "                x1 = image_features[filename].reshape(-1,)\n",
    "                x2 = np.array(sample[:i + 1])\n",
    "\n",
    "                combined_X = np.concatenate([x1, x2])\n",
    "\n",
    "                y = to_categorical(sample[i+1], VOCAB_SIZE)\n",
    "            \n",
    "                all_Xs[i].append(combined_X)\n",
    "                all_ys[i].append(y)\n",
    "\n",
    "\n",
    "    all_Xs = [tf.convert_to_tensor(samples) for samples in all_Xs]\n",
    "    all_ys = [tf.convert_to_tensor(samples) for samples in all_ys]\n",
    "    \n",
    "    return all_Xs, all_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change and comment\n",
    "\n",
    "def create_sequences(image_features, descriptions, desired_caption_size):\n",
    "    '''\n",
    "    given 5 descriptions corresponding to one image, output a list of:\n",
    "    (image feature vector, first i-1 words in a sequence, ith word) \n",
    "    '''\n",
    "    X = []\n",
    "    # next word\n",
    "    y = []\n",
    "\n",
    "    image_features = image_features.reshape(-1,)\n",
    "                                \n",
    "    descriptions = tokenizer.texts_to_sequences(descriptions)\n",
    "    for description in descriptions:\n",
    "        caption = np.array(description[:desired_caption_size])\n",
    "        combined_X = np.concatenate([image_features, caption])\n",
    "        \n",
    "        X.append(combined_X)\n",
    "        y.append(to_categorical(description[:desired_caption_size], VOCAB_SIZE)[0])\n",
    "\n",
    "    return tf.convert_to_tensor(X), tf.convert_to_tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change and comment\n",
    "\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(filename_description_dictionary, img_features_dict, desired_caption_size, loops):\n",
    "    while loops >= 1:\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "\n",
    "        # shuffle filename order for better distribution over multiple loops (epochs)\n",
    "        all_filenames = list(filename_description_dictionary.keys())\n",
    "        np.random.shuffle(all_filenames)\n",
    "        \n",
    "        # loop for ever over files\n",
    "        for filename in all_filenames:\n",
    "            # get the corresponding descriptions\n",
    "            descriptions = filename_description_dictionary[filename]\n",
    "\n",
    "            # retrieve the photo feature\n",
    "            img_features = img_features_dict[filename][0]\n",
    "\n",
    "            input_vector, out_word = create_sequences(img_features, descriptions, desired_caption_size)\n",
    "            yield input_vector, out_word\n",
    "\n",
    "        loops -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ReLU\n",
    "\n",
    "def generate_logistic_model(input_size):\n",
    "\n",
    "    linear_activation = ReLU(negative_slope=1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_size, activation=linear_activation))\n",
    "\n",
    "    model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class LogisticDecoder():\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    def __init__(self, caption_max_length, solver='liblinear'):\n",
    "        '''\n",
    "        '''\n",
    "        self.max_len = caption_max_length\n",
    "\n",
    "        # generate a model that takes in an image feature vector, and the caption so far, and outputs the next word\n",
    "        # each model corresponds to a specefic input caption length\n",
    "        self.models = [generate_logistic_model(4096 + i + 1) for i in range(caption_max_length)]\n",
    "\n",
    "\n",
    "    def fit(self, sample_dictionary, image_feature_dictionary, verbose=False):\n",
    "        samples_x, samples_y = dictionary_to_model_samples(sample_dictionary, image_feature_dictionary, max_length=MAX_LENGTH)\n",
    "\n",
    "        if verbose:\n",
    "            print('Samples Generated, beginning training')\n",
    "        \n",
    "\n",
    "        for i in range(self.max_len):\n",
    "            current_x = samples_x[i]\n",
    "            current_y = samples_y[i]\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Training model #{i+1}')\n",
    "\n",
    "            self.models[i].fit(current_x, current_y)\n",
    "\n",
    "    def prediction(self, image_feature):\n",
    "        for i in range(self.max_len):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_decoder = LogisticDecoder(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = logistic_decoder.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_generator = data_generator(validation_dict, val_image_features, 1, loops=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toyso\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 125s 154ms/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c211bd0ec8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.fit_generator(first_generator, epochs=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0184f448d6494873b5885b7cafa76c11f0e318a0940098d9222ca8536d4b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

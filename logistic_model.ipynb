{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import ReLU\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from common import RANDOM_SEED, clean_descriptions, ALL_FILENAMES\n",
    "from common import samples_to_dict, import_image_features, max_and_average_sequence_length, START_TOK, END_TOK, get_tokenizer_from_samples, reset_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image                              1000268201_693b08cb0e.jpg\n",
      "caption    [<start>, a, child, in, a, pink, dress, is, cl...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# import the cleaned data and print one example\n",
    "cleaned_data = clean_descriptions('data/flickr_8k/captions.txt')\n",
    "print(cleaned_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "get the actual rows from the df corresponding to the different sets\n",
    "'''\n",
    "# get the samples with the given filenames\n",
    "train_samples = clean_descriptions('data/flickr_8k/train.csv')\n",
    "validation_samples = clean_descriptions('data/flickr_8k/validation.csv')\n",
    "test_samples = clean_descriptions('data/flickr_8k/test.csv')\n",
    "train_and_val_samples = clean_descriptions('data/flickr_8k/train_and_val.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                    0\n",
       "image                                 1000268201_693b08cb0e.jpg\n",
       "caption       [<start>, a, child, in, a, pink, dress, is, cl...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_dict = samples_to_dict(train_samples)\n",
    "validation_dict = samples_to_dict(validation_samples)\n",
    "train_and_val_dict = samples_to_dict(train_and_val_samples)\n",
    "test_dict = samples_to_dict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_features = import_image_features('8k_features.pkl', ALL_FILENAMES)\n",
    "\n",
    "# NOTE: we dont use these anymore and just use the one all_image_features variable to get the features from\n",
    "# train_image_features = import_image_features('8k_features.pkl', TRAIN_FILENAMES)\n",
    "# val_image_features = import_image_features('8k_features.pkl', VALIDATION_FILENAMES)\n",
    "# train_and_val_image_features = import_image_features('8k_features.pkl', TRAIN_AND_VAL_FILENAMES)\n",
    "# test_image_features = import_image_features('8k_features.pkl', TEST_FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[37, 35, 35, 35, 34, 33, 33, 33, 33, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30]\n",
      "The longest sequence length from the training and validation samples is 37\n",
      "The average sequence length from the training and validation samples is 12\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH, AVG_LENGTH = max_and_average_sequence_length(train_and_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_of_specific_size(image_vector, descriptions, desired_caption_size, tokenizer):\n",
    "    '''\n",
    "    Given:\n",
    "    - one image vector representing a single image\n",
    "    - one value from our samples dict for the corresponding image (a list of 5 tokenized captions)\n",
    "    - the desired caption size N\n",
    "    - a Tokenizer\n",
    "\n",
    "    Return X:\n",
    "        a tensor with 5 elements (one for each caption) where each item is an array with length 4096 + N\n",
    "        - the first 4096 elements are the VGG extracted features of the corresponding image\n",
    "        - the next N elements are the first N words of the caption (converted to numbers by the passed in tokenizer)\n",
    "    And Y:\n",
    "        a tensor with 5 elements where each element is:\n",
    "        - the N+1 word in the sequence\n",
    "\n",
    "    NOTE: If any caption has a total length less than or equal to N, then it will not be added to the output, \n",
    "    meaning that the output could be tensors with 0 dimensions\n",
    "    '''\n",
    "    # initalize empty arrays for the samples\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # reshape the image vector to be one dimension\n",
    "    image_vector = image_vector.reshape(-1,)\n",
    "                                \n",
    "    # convert the descriptions to number lists instead of string lists\n",
    "    descriptions = tokenizer.texts_to_sequences(descriptions)\n",
    "    \n",
    "    # get vocab size from tokenizer\n",
    "    vocab_size = len(tokenizer.index_word) + 1\n",
    "\n",
    "    # for each description\n",
    "    for description in descriptions:\n",
    "        # only if the caption is at least 1 longer than N (desired_caption_size)\n",
    "        if len(description) >= desired_caption_size + 1:\n",
    "            # get the caption as a numpy array of elements (length will be desired caption size)\n",
    "            caption = np.array(description[:desired_caption_size])\n",
    "        \n",
    "            # concatenate the image vector with the caption vector\n",
    "            combined_X = np.concatenate([image_vector, caption])\n",
    "\n",
    "            # append the combined X vector to the output x list\n",
    "            X.append(combined_X)\n",
    "\n",
    "            # get the one-hot encoding of the last word (required for keras models)\n",
    "            # and append it to the output y list\n",
    "            last_word = description[desired_caption_size]\n",
    "            \n",
    "            y.append(to_categorical(last_word, vocab_size))\n",
    "\n",
    "    return tf.convert_to_tensor(X), tf.convert_to_tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(filename_description_dictionary, desired_caption_size, loops, tokenizer):\n",
    "    '''\n",
    "    Given:\n",
    "    - A dictionary containing the samples we want to create a generator for where\n",
    "        key: filename (string)\n",
    "        value: list of captions (where each caption is a list of strings)\n",
    "    - A desired caption size (N)\n",
    "    - A number of loops L\n",
    "    - A tokenizer for converting seen words to numbers\n",
    "    \n",
    "    loops are used because a generator is expended once it yields its last result, and \n",
    "    therefore cannot be used over multiple epohchs\n",
    "\n",
    "    Iterate through the filename_description dictionary (L times).\n",
    "    For each filename, generate corresponding number of samples where the caption size is N\n",
    "    Each value in the X samples vector will be 4096 + N\n",
    "        Those N values are the first N words of the corresponding caption\n",
    "    Each value in the y vector will be the N+1 word\n",
    "\n",
    "    Used to save memory\n",
    "    Each loop it shuffles the order of the samples\n",
    "    '''\n",
    "    for _ in range(loops):\n",
    "        # shuffle filename order for better distribution over multiple loops (epochs)\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        all_filenames = list(filename_description_dictionary.keys())\n",
    "        np.random.shuffle(all_filenames)\n",
    "        \n",
    "        # loop for ever over filenames\n",
    "        for filename in all_filenames:\n",
    "            # get the corresponding descriptions\n",
    "            descriptions = filename_description_dictionary[filename]\n",
    "\n",
    "            # retrieve the image feature vector\n",
    "            image_vector = all_image_features[filename][0]\n",
    "\n",
    "            # get the samples of the desired size (N) \n",
    "            x_samples, y_samples = get_samples_of_specific_size(image_vector, descriptions, desired_caption_size, tokenizer)\n",
    "            \n",
    "            # if there are no samples of that shape\n",
    "            if y_samples.shape == (0,):\n",
    "                # continue the loop until there are\n",
    "                continue\n",
    "            \n",
    "            yield x_samples, y_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REMOVE IF NOT USED\n",
    "\n",
    "# def dictionary_to_model_samples(dictionary, image_features, max_length):\n",
    "#     # list of image features concatenated with corresponding first i-1 words\n",
    "#     all_Xs = [[] for i in range(max_length)]\n",
    "\n",
    "#     # next word for corresponding sentence\n",
    "#     all_ys = [[] for i in range(max_length)]\n",
    "\n",
    "#     for filename, samples in dictionary.items():\n",
    "#         samples = tokenizer.texts_to_sequences(samples)\n",
    "#         for sample in samples:\n",
    "#             for i in range(len(sample) - 1):\n",
    "\n",
    "#                 if i > max_length - 1:\n",
    "#                     break \n",
    "\n",
    "#                 x1 = image_features[filename].reshape(-1,)\n",
    "#                 x2 = np.array(sample[:i + 1])\n",
    "\n",
    "#                 combined_X = np.concatenate([x1, x2])\n",
    "\n",
    "#                 y = to_categorical(sample[i+1], VOCAB_SIZE)\n",
    "            \n",
    "#                 all_Xs[i].append(combined_X)\n",
    "#                 all_ys[i].append(y)\n",
    "\n",
    "\n",
    "#     all_Xs = [tf.convert_to_tensor(samples) for samples in all_Xs]\n",
    "#     all_ys = [tf.convert_to_tensor(samples) for samples in all_ys]\n",
    "    \n",
    "#     return all_Xs, all_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logistic_model(input_size, output_size):\n",
    "    '''\n",
    "    generate a logistic regression model using keras api\n",
    "    \n",
    "    since our model uses multiple logistic regression models, \n",
    "    we wanted to run it on the gpu which is simple with keras\n",
    "    '''\n",
    "    # create a linear activation function, relu which doesn't punish values < 0\n",
    "    linear_activation = ReLU(negative_slope=1)\n",
    "\n",
    "    # FF NN\n",
    "    model = Sequential()\n",
    "\n",
    "    # input layer is given input size (4096 + number of words for corresponding decoder)\n",
    "    model.add(Dense(input_size, activation=linear_activation))\n",
    "\n",
    "    # output layer with softmax for the whole vocabulary\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "    # compile and return\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticDecoder():\n",
    "    '''\n",
    "    this class represents a model that can generate captions for text based on image input\n",
    "\n",
    "    it works by creating a logistic regression classifier for each position in the output string\n",
    "\n",
    "    each logistic regression model is assigned an index corresponding to the number of words it takes as input\n",
    "    for example:\n",
    "        model at index 3 is responsible for taking in a feature vector of length 4096 + 3\n",
    "        the first 4096 values are the image input, and the +3 represents the first 3 strings in the caption\n",
    "        the model predicts the 4th word\n",
    "    '''\n",
    "\n",
    "    def __init__(self, caption_max_length, tokenizer):\n",
    "        '''\n",
    "        given a max caption length N, initalize N logistic regression models, one for each position in our caption length\n",
    "        given a tokenizer, store it in the model\n",
    "        '''\n",
    "        self.max_len = caption_max_length\n",
    "\n",
    "        # store the tokenizer for later use\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # get the vocab size (add one due to the way keras tokenizer works)\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # generate a model that takes in an image feature vector, and the caption so far, and outputs the next word\n",
    "        self.models = [None for i in range(caption_max_length)]\n",
    "\n",
    "\n",
    "    def fit(self, sample_dictionary, epochs, model_save_directory, verbose=False):\n",
    "        '''\n",
    "        given a dictionary of samples (key is a filename and value is all associated captions tokenized into lists of strings)\n",
    "        and a number of epochs\n",
    "\n",
    "        train the logistic decoders to generate captions\n",
    "\n",
    "        if model_save_directory is given, save the logistic models into the given directory\n",
    "        don't add a / at the end of the directory\n",
    "        '''\n",
    "        for i in range(self.max_len):\n",
    "            if verbose:\n",
    "                print(f'Training model #{i+1}')\n",
    "\n",
    "            current_generator = data_generator(sample_dictionary, desired_caption_size=i+1, loops=epochs, tokenizer=self.tokenizer)\n",
    "            \n",
    "            current_model = generate_logistic_model(4096 + i + 1, self.vocab_size)\n",
    "\n",
    "            current_model.fit_generator(current_generator)\n",
    "\n",
    "            # save the model to a designated parent folder\n",
    "            save_path = f'{model_save_directory}/decoder{i+1}'\n",
    "            current_model.save(save_path)\n",
    "            \n",
    "            # clear memory for next model\n",
    "            reset_keras(current_model)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Model #{i+1} saved to {save_path}')\n",
    "                \n",
    "            # TODO save tokenizer as well and load it in the load function \n",
    "              \n",
    "        # after training, reload all saved models\n",
    "        self.load(model_save_directory)\n",
    "                    \n",
    "    def load(self, directory_path):\n",
    "        '''\n",
    "        load in a model from a folder that has all decoders saved into it\n",
    "        \n",
    "        do not add a / at the end of the directory path\n",
    "        '''\n",
    "        for i in range(self.max_len):\n",
    "            self.models[i] = load_model(f'{directory_path}/decoder{i+1}')\n",
    "            \n",
    "        print(f'Model loaded from {directory_path}')\n",
    "        \n",
    "                \n",
    "    def generate_caption(self, image_filename, verbose=True):\n",
    "        '''\n",
    "        given a filename use the trained models to decode each next word for a full caption\n",
    "        '''\n",
    "        caption = [START_TOK]\n",
    "\n",
    "        image_vector = all_image_features[image_filename].reshape(-1,)\n",
    "\n",
    "        for i in range(self.max_len):\n",
    "            # should be length i + 1 because one word is added each iteration\n",
    "            caption_as_indices = self.tokenizer.texts_to_sequences([caption])[0] \n",
    "            \n",
    "            # should be length 4096 + (i+1) for the input of the corresponding decoder\n",
    "            next_input = np.concatenate([image_vector, np.array(caption_as_indices)])\n",
    "            \n",
    "            # reshape it into 1 x 4096 + (i+1) shape for keras input\n",
    "            next_input = next_input.reshape(1, len(next_input))\n",
    "\n",
    "            # get the current model\n",
    "            current_model = self.models[i]\n",
    "            \n",
    "            # get the probability distribution for output layer\n",
    "            probablities = current_model.predict(next_input).reshape(-1,)\n",
    "                    \n",
    "            # predict the index of the next word (randomly sample from the vocab based on the prediction output distribution)\n",
    "            predicted_word_index = np.random.choice(self.vocab_size, p=probablities)\n",
    "            \n",
    "            # convert the index to a word based on the tokenizer\n",
    "            predicted_word = self.tokenizer.index_word[predicted_word_index]\n",
    "\n",
    "            # add the word to out caption\n",
    "            caption.append(predicted_word)\n",
    "\n",
    "            # if it is the end of sequence token break out the loop\n",
    "            if predicted_word == END_TOK:\n",
    "                break\n",
    "        \n",
    "        if verbose:\n",
    "            # TODO come back and make caption into a string\n",
    "            print(f'Caption for {image_filename}: {caption}')\n",
    "\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer_from_samples(train_and_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we choose 12 as our max caption length, despite there being many longer captions,\n",
    "computationally we were limited on time, and therefore chose to decrease the number of decoders\n",
    "to the average caption length rather than the maximum seen caption length\n",
    "'''\n",
    "logistic_decoder = LogisticDecoder(12, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from LogisticDecoders/train_val_20_epoch_maxlen_12\n"
     ]
    }
   ],
   "source": [
    "# logistic_decoder.load('LogisticDecoders/train_val_20_epoch_maxlen_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption = logistic_decoder.generate_caption('111766423_4522d36e56.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8523, (8497,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic_decoder.vocab_size, caption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' and 'p' must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3428\\2851347059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogistic_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'a' and 'p' must have same size"
     ]
    }
   ],
   "source": [
    "# np.random.choice(logistic_decoder.vocab_size, p=caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_decoder.fit(train_and_val_dict, epochs=20, model_save_directory='', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = list(data_generator(validation_dict, 1, 1, tokenizer))\n",
    "\n",
    "# x = [item[0].numpy() for item in check]\n",
    "# y = [item[1].numpy() for item in check]\n",
    "\n",
    "\n",
    "# output_x = []\n",
    "# output_y = []\n",
    "# for itemsx, itemsy in zip(x, y):\n",
    "#     for itemx, itemy in zip(itemsx, itemsy):\n",
    "#         output_x.append(itemx)\n",
    "#         output_y.append(itemy.argmax())\n",
    "        \n",
    "# x = output_x\n",
    "# y = output_y\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# test = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "# test.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption = logistic_decoder.generate_caption('111766423_4522d36e56.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = LogisticDecoder(5, tokenizer)\n",
    "# new_model.load('LogisticDecoders/train_val_1_epoch_maxlen_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption = new_model.generate_caption('111766423_4522d36e56.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0184f448d6494873b5885b7cafa76c11f0e318a0940098d9222ca8536d4b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.9.1\n",
      "keras: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorflow: {tf.__version__}')\n",
    "print(f'keras: {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code borrowed mostly from https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    " \n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory_to_extract, feature_dump_location=None, verbose=False):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "\n",
    "    # re-structure the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "    # summarize\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    \n",
    "    for name in listdir(directory_to_extract):\n",
    "        # load an image from file\n",
    "        filename = directory_to_extract + '/' + name\n",
    "        \n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        if verbose:\n",
    "            print(name)\n",
    "\n",
    "    print(f'Extracted Features: {len(features)}')\n",
    "\n",
    "    if feature_dump_location:\n",
    "        dump(features, open(feature_dump_location, 'wb'))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features: 10\n"
     ]
    }
   ],
   "source": [
    "test_features = extract_features('data/test_10_images/', 'test_10_images_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features('data/flickr_8k/Images/', 'flickr_8k/8k_features.pkl', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features('data/flickr_30k/Images/', 'flickr_30k/30k_features.pkl', verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ba28e5224b9cfd8866d2b1b5c5686f8caa4fc33c52e451bf4c769cd7356f3e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

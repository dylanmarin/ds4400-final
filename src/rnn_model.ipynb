{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This file shows an example of how to create, train, load, and generate captions with our \n",
    "RNNModel class\n",
    "\n",
    "previously it stored all of the code for the class but we extracted it to LogisticDecoder.py for use in other files\n",
    "\n",
    "'''\n",
    "from common import clean_descriptions, samples_to_dict, VALIDATION_FILENAMES\n",
    "from RNNDecoder import RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "get the actual rows from the df corresponding to the different sets\n",
    "'''\n",
    "training_samples = clean_descriptions('../data/flickr_8k/train.csv')\n",
    "validation_samples = clean_descriptions('../data/flickr_8k/validation.csv')\n",
    "test_samples = clean_descriptions('../data/flickr_8k/test.csv')\n",
    "train_and_val_samples = clean_descriptions('../data/flickr_8k/train_and_val.csv')\n",
    "small_train_samples = clean_descriptions('../data/flickr_8k/small_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get the samples as dictionaries from image filename to cleaned captions\n",
    "'''\n",
    "training_dict = samples_to_dict(training_samples)\n",
    "validation_dict = samples_to_dict(validation_samples)\n",
    "test_dict = samples_to_dict(test_samples)\n",
    "train_and_val_dict = samples_to_dict(train_and_val_samples)\n",
    "small_train_dict = samples_to_dict(small_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "example of how to create, train, and load an RNN Model\n",
    "\n",
    "we will compare these against each other in a separate file (model_tuning.ipynb)\n",
    "\n",
    "using the small training set for this comparison\n",
    "'''\n",
    "\n",
    "# set dropout_layer to False for 0 dropout layers\n",
    "rnn_model = RNNModel(dropout_layer=False, samples=small_train_samples, optimizer='adam')\n",
    "\n",
    "# set dropout_layer to True for dropout layers\n",
    "rnn_model = RNNModel(dropout_layer=True, samples=small_train_samples, optimizer='adam')\n",
    "\n",
    "# rnn_model_with_dropout.train_save_model(input_dict=small_train_dict, save_path='../models/example/rnn_no_dropout', epochs=1)\n",
    "rnn_model.load('../models/example/rnn_no_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.generate_caption(image_filename='387974450_bcd205daac', verbose=True)\n",
    "rnn_model.generate_captions_for_files(filenames=VALIDATION_FILENAMES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0184f448d6494873b5885b7cafa76c11f0e318a0940098d9222ca8536d4b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

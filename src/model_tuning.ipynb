{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this file we train many models and evaluate their caption predictions on the validation and test sets\n",
    "\n",
    "This file will also be used to generate some captions for the report\n",
    "\n",
    "All data gathered is in the report\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from LogisticDecoder import LogisticDecoder\n",
    "from common import clean_descriptions, samples_to_dict, corpus_bleu_score\n",
    "from RNNDecoder import RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Initialize the samples and dictionaries that will be used in training\n",
    "\n",
    "'''\n",
    "# get the samples with the given filenames\n",
    "small_train_samples = clean_descriptions('../data/flickr_8k/small_train.csv')\n",
    "validation_samples = clean_descriptions('../data/flickr_8k/validation.csv')\n",
    "train_samples = clean_descriptions('../data/flickr_8k/train.csv')\n",
    "test_samples = clean_descriptions('../data/flickr_8k/test.csv')\n",
    "train_and_val_samples = clean_descriptions('../data/flickr_8k/train_and_val.csv')\n",
    "\n",
    "\n",
    "small_train_dict = samples_to_dict(small_train_samples)\n",
    "train_dict = samples_to_dict(train_samples)\n",
    "test_dict = samples_to_dict(test_samples)\n",
    "train_and_val_dict = samples_to_dict(train_and_val_samples)\n",
    "validation_dict = samples_to_dict(validation_samples)\n",
    "\n",
    "# get the captions for the validation set and the test set\n",
    "VALIDATION_FILENAMES = list(validation_dict.keys())\n",
    "val_captions = list(validation_dict.values())\n",
    "\n",
    "# not using predefined variable because the order is different and it matters for how we generate captions and calculate bleu scores\n",
    "TEST_FILENAMES = list(test_dict.keys())\n",
    "test_captions = list(test_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/compare_3_models/LogisticModel\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following 3 models are used to select which model is best. We will then tune hyper parameters using that model structure\n",
    "\n",
    "create logistic regression model\n",
    "5 epochs\n",
    "small training set\n",
    "evaluate on validation set\n",
    "'''\n",
    "logistic_decoder = LogisticDecoder(15, small_train_samples)\n",
    "# logistic_decoder.fit(small_train_dict, 5, '../models/compare_3_models/LogisticModel',verbose=True)\n",
    "logistic_decoder.load('../models/compare_3_models/LogisticModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019D99247798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019D99273798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "BLEU-1: 0.3688110474907376\n",
      "BLEU-2: 0.11844459557724961\n",
      "BLEU-3: 0.040301407647280026\n",
      "BLEU-4: 0.010024117672673933\n"
     ]
    }
   ],
   "source": [
    "logistic_captions = logistic_decoder.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "logistic_bleu_scores = corpus_bleu_score(val_captions, logistic_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 32, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "create RNN model without dropout layers\n",
    "5 epochs\n",
    "small training set\n",
    "evaluate on validation set\n",
    "'''\n",
    "rnn_without_dropout_model = RNNModel(False, small_train_samples)\n",
    "# rnn_without_dropout_model.train_save_model(input_dict=small_train_dict, save_path='../models/compare_3_models/RNN_without_dropout', epochs=5)\n",
    "rnn_without_dropout_model.load('../models/compare_3_models/RNN_without_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.36882039122703025\n",
      "BLEU-2: 0.16086606150804358\n",
      "BLEU-3: 0.06777855153160156\n",
      "BLEU-4: 0.027481417559851205\n"
     ]
    }
   ],
   "source": [
    "without_dropout_captions = rnn_without_dropout_model.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "without_dropout_bleu_scores = corpus_bleu_score(val_captions, without_dropout_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 32, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "create RNN model with dropout layers\n",
    "5 epochs\n",
    "small training set\n",
    "evaluate on validation set\n",
    "'''\n",
    "rnn_with_dropout_model = RNNModel(True, small_train_samples)\n",
    "# rnn_with_dropout_model.train_save_model(input_dict=small_train_dict, save_path='../models/compare_3_models/RNN_with_dropout', epochs=5)\n",
    "rnn_with_dropout_model.load('../models/compare_3_models/RNN_with_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3708819200724556\n",
      "BLEU-2: 0.16598642265091337\n",
      "BLEU-3: 0.07421146275530921\n",
      "BLEU-4: 0.03623250071970148\n"
     ]
    }
   ],
   "source": [
    "with_dropout_captions = rnn_with_dropout_model.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "with_dropout_bleu_scores = corpus_bleu_score(val_captions, with_dropout_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 32, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Note:\n",
    "\n",
    "best model was the RNN model with dropout layers\n",
    "\n",
    "now do hyperparameter tuning on best model\n",
    "\n",
    "optimizer   | # epochs\n",
    "adam        | 5\n",
    "adam        | 10\n",
    "sgd         | 5\n",
    "sgd         | 10\n",
    "\n",
    "\n",
    "evaluate each model on the validation set\n",
    "'''\n",
    "adam_5_epochs = RNNModel(True, small_train_samples, optimizer='adam')\n",
    "# adam_5_epochs.train_save_model(input_dict=small_train_dict, save_path='../models/hyperparameter_tuning/rnn_5_epochs_adam', epochs=5)\n",
    "adam_5_epochs.load('../models/hyperparameter_tuning/rnn_5_epochs_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3481897215802795\n",
      "BLEU-2: 0.15029988165849215\n",
      "BLEU-3: 0.06072035219055026\n",
      "BLEU-4: 0.024662053950138443\n"
     ]
    }
   ],
   "source": [
    "adam_5_captions = adam_5_epochs.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "adam_5_bleu_scores = corpus_bleu_score(val_captions, adam_5_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 32, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RNN model with dropout \n",
    "adam optimizer 10 epochs\n",
    "'''\n",
    "adam_10_epochs = RNNModel(True, small_train_samples, optimizer='adam')\n",
    "# adam_10_epochs.train_save_model(input_dict=small_train_dict, save_path='../models/hyperparameter_tuning/rnn_10_epochs_adam', epochs=10)\n",
    "adam_10_epochs.load('../models/hyperparameter_tuning/rnn_10_epochs_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.36857142857142855\n",
      "BLEU-2: 0.1693732493313534\n",
      "BLEU-3: 0.07620747950748036\n",
      "BLEU-4: 0.036732043324973485\n"
     ]
    }
   ],
   "source": [
    "adam_10_captions = adam_10_epochs.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "adam_10_bleu_scores = corpus_bleu_score(val_captions, adam_10_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 32, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RNN model with dropout \n",
    "sgd optimizer 5 epochs\n",
    "'''\n",
    "sgd_5_epochs = RNNModel(True, small_train_samples, optimizer='sgd')\n",
    "# sgd_5_epochs.train_save_model(input_dict=small_train_dict, save_path='../models/hyperparameter_tuning/rnn_5_epochs_sgd', epochs=5)\n",
    "sgd_5_epochs.load('../models/hyperparameter_tuning/rnn_5_epochs_sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toyso\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.26977904490377763\n",
      "BLEU-2: 0.06311048157167536\n",
      "BLEU-3: 0.009384591087888932\n",
      "BLEU-4: 3.6825412758162053e-79\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RNN model with dropout \n",
    "sgd optimizer 5 epochs\n",
    "'''\n",
    "sgd_5_captions = sgd_5_epochs.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "sgd_5_bleu_scores = corpus_bleu_score(val_captions, sgd_5_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 32, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RNN model with dropout \n",
    "sgd optimizer 10 epochs\n",
    "'''\n",
    "sgd_10_epochs = RNNModel(True, small_train_samples, optimizer='sgd')\n",
    "# sgd_10_epochs.train_save_model(input_dict=small_train_dict, save_path='../models/hyperparameter_tuning/rnn_10_epochs_sgd', epochs=10)\n",
    "sgd_10_epochs.load('../models/hyperparameter_tuning/rnn_10_epochs_sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3113117620450892\n",
      "BLEU-2: 0.08366985387481277\n",
      "BLEU-3: 0.013668175954438951\n",
      "BLEU-4: 4.7288634603256347e-79\n"
     ]
    }
   ],
   "source": [
    "sgd_10_captions = sgd_10_epochs.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "sgd_10_bleu_scores = corpus_bleu_score(val_captions, sgd_10_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 33, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The next 3 models are ALL THREE models trained with the best hyperparameters\n",
    "\n",
    "adam optimizer with 10 epochs\n",
    "\n",
    "First the RNN model with dropout layers\n",
    "Second the RNN model without dropout layers\n",
    "Lastly the Logistic Decoder model\n",
    "\n",
    "All will be tested on the validation set again\n",
    "'''\n",
    "full_train_10_epochs = RNNModel(True, train_samples, optimizer='adam')\n",
    "# full_train_10_epochs.train_save_model(input_dict=train_dict, save_path='../models/full_training_data/rnn_10_epochs_adam', epochs=10)\n",
    "full_train_10_epochs.load('../models/full_training_data/rnn_10_epochs_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.36541236541236544\n",
      "BLEU-2: 0.16888892197921862\n",
      "BLEU-3: 0.07157808723253177\n",
      "BLEU-4: 0.03164923529849848\n"
     ]
    }
   ],
   "source": [
    "full_train_10_captions = full_train_10_epochs.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "full_train_10_bleu_scores = corpus_bleu_score(val_captions, full_train_10_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 33, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RNN model without dropout\n",
    "'''\n",
    "full_train_without_dropout_10_epochs = RNNModel(False, train_samples, optimizer='adam')\n",
    "# full_train_without_dropout_10_epochs.train_save_model(input_dict=train_dict, save_path='../models/full_training_data/no_dropout_10_epochs_adam', epochs=10)\n",
    "full_train_without_dropout_10_epochs.load('../models/full_training_data/no_dropout_10_epochs_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3501296358922331\n",
      "BLEU-2: 0.15442092433130808\n",
      "BLEU-3: 0.06434277732239087\n",
      "BLEU-4: 0.02597076238778425\n"
     ]
    }
   ],
   "source": [
    "full_train_without_dropout_10_captions = full_train_without_dropout_10_epochs.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "full_train_without_dropout_10_bleu_scores = corpus_bleu_score(val_captions, full_train_without_dropout_10_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/full_training_data/logistic_model\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "logistic model trained on training set\n",
    "'''\n",
    "full_train_logistic_decoder = LogisticDecoder(15, train_samples)\n",
    "# full_train_logistic_decoder.fit(train_dict, 10, '../models/full_training_data/logistic_model',verbose=True)\n",
    "full_train_logistic_decoder.load('../models/full_training_data/logistic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9684 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002265ABFD8B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 9685 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002265AC8DAF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "BLEU-1: 0.4127408397371645\n",
      "BLEU-2: 0.14567295125330063\n",
      "BLEU-3: 0.048268953218352444\n",
      "BLEU-4: 0.01710742831848821\n"
     ]
    }
   ],
   "source": [
    "full_train_logistic_captions = full_train_logistic_decoder.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "full_train_logistic_bleu_scores = corpus_bleu_score(val_captions, full_train_logistic_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 35, 34, 33, 33, 33, 33, 33, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train all 3 model types on full training + validation set with best hyper parameters\n",
    "\n",
    "use these just for testing some caption generation\n",
    "'''\n",
    "\n",
    "full_train_val_rnn_10_epochs = RNNModel(True, train_and_val_samples, optimizer='adam')\n",
    "# full_train_val_rnn_10_epochs.train_save_model(input_dict=train_and_val_dict, save_path='../models/full_training_val_data/rnn_10_epochs_adam', epochs=10)\n",
    "full_train_val_rnn_10_epochs.load('../models/full_training_val_data/rnn_10_epochs_adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3578531948223495\n",
      "BLEU-2: 0.15077712216509875\n",
      "BLEU-3: 0.06486047253678623\n",
      "BLEU-4: 0.028643511685858013\n"
     ]
    }
   ],
   "source": [
    "full_train_val_rnn_10_captions = full_train_val_rnn_10_epochs.generate_captions_for_files(TEST_FILENAMES, verbose=False)\n",
    "full_train_val_10_bleu_scores = corpus_bleu_score(test_captions, full_train_val_rnn_10_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 35, 34, 33, 33, 33, 33, 33, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RNN model without dropout on training + validation set\n",
    "'''\n",
    "full_train_val_without_dropout_10_epochs = RNNModel(False, train_and_val_samples, optimizer='adam')\n",
    "# full_train_val_without_dropout_10_epochs.train_save_model(input_dict=train_and_val_dict, save_path='../models/full_training_val_data/no_dropout_10_epochs_adam', epochs=10)\n",
    "full_train_val_without_dropout_10_epochs.load('../models/full_training_val_data/no_dropout_10_epochs_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3571275225418635\n",
      "BLEU-2: 0.160156308197626\n",
      "BLEU-3: 0.07407812945502383\n",
      "BLEU-4: 0.03210307147322879\n"
     ]
    }
   ],
   "source": [
    "full_train_val_without_dropout_10_captions = full_train_val_without_dropout_10_epochs.generate_captions_for_files(TEST_FILENAMES, verbose=False)\n",
    "full_train_val_without_dropout_10_bleu_scores = corpus_bleu_score(test_captions, full_train_val_without_dropout_10_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/full_training_val_data/logistic_model\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "logistic model trained on training + validation set\n",
    "'''\n",
    "full_train_val_logistic_decoder = LogisticDecoder(15, train_and_val_samples)\n",
    "# full_train_val_logistic_decoder.fit(train_and_val_dict, 10, '../models/full_training_val_data/logistic_model',verbose=True)\n",
    "full_train_val_logistic_decoder.load('../models/full_training_val_data/logistic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 10129 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E4609CEB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 10130 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E46084FAF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "BLEU-1: 0.4072196938353331\n",
      "BLEU-2: 0.135258190075289\n",
      "BLEU-3: 0.04907325539219344\n",
      "BLEU-4: 0.019574905251922445\n"
     ]
    }
   ],
   "source": [
    "full_train_val_logistic_captions = full_train_val_logistic_decoder.generate_captions_for_files(TEST_FILENAMES, verbose=False)\n",
    "full_train_val_logistic_bleu_scores = corpus_bleu_score(test_captions, full_train_val_logistic_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "combine and export all captions generated by our 3 fully trained models for viewing\n",
    "'''\n",
    "def concat_string(tokens):\n",
    "    output = ''\n",
    "    for i, tok in enumerate(tokens):\n",
    "        output += tok\n",
    "        if i < len(tokens)-1:\n",
    "            output += ' '\n",
    "    return output\n",
    "\n",
    "def convert_lists(lists_of_toks):\n",
    "    '''\n",
    "    remove <start> and <end> from true labels and reappend them to the list\n",
    "    '''\n",
    "    output = []\n",
    "    for token_list in lists_of_toks:\n",
    "        token_list = token_list[1:len(token_list)-1]\n",
    "        output.append(concat_string(token_list))\n",
    "    return output\n",
    "\n",
    "# commented to not overwrite files\n",
    "# pd.concat([\n",
    "#     pd.Series(test_captions, name='True Captions', index=TEST_FILENAMES).apply(convert_lists),\n",
    "#     pd.Series(full_train_val_rnn_10_captions, name='RNN With Dropout Layers', index=TEST_FILENAMES).apply(concat_string),\n",
    "#     pd.Series(full_train_val_without_dropout_10_captions, name='RNN Without Dropout', index=TEST_FILENAMES).apply(concat_string),\n",
    "#     pd.Series(full_train_val_logistic_captions, name='LogisticDecoder', index=TEST_FILENAMES).apply(concat_string)\n",
    "# ], axis=1).to_csv('../data/generated_captions.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below this cell are just some extra experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 sequence lengths are:\n",
      "[35, 35, 34, 33, 33, 33, 33, 33, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n",
      "The longest sequence length from the training and validation samples is 35\n",
      "The average sequence length from the training and validation samples is 12\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "original goal for this model was to train it overnight on the entire training + validation set\n",
    "accidentally trained it on only training set\n",
    "'''\n",
    "\n",
    "full_train_150_epochs = RNNModel(True, train_samples, optimizer='adam')\n",
    "# full_train_150_epochs.train_save_model(input_dict=train_dict, save_path='../models/full_training_data/with_dropout_150_epochs_adam', epochs=150)\n",
    "full_train_150_epochs.load('../models/full_training_data/with_dropout_150_epochs_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.34854186265286924\n",
      "BLEU-2: 0.13711198910620406\n",
      "BLEU-3: 0.06145770081029022\n",
      "BLEU-4: 0.02654907368336846\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "scores on validation set\n",
    "'''\n",
    "full_train_150_val_captions = full_train_150_epochs.generate_captions_for_files(VALIDATION_FILENAMES, verbose=False)\n",
    "full_train_150_val_bleu_scores = corpus_bleu_score(val_captions, full_train_150_val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3364551627253571\n",
      "BLEU-2: 0.12992880857508834\n",
      "BLEU-3: 0.05701062669647279\n",
      "BLEU-4: 0.025052644977417254\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "scores on test set\n",
    "'''\n",
    "full_train_150_test_captions = full_train_150_epochs.generate_captions_for_files(TEST_FILENAMES, verbose=False)\n",
    "full_train_150_test_bleu_scores = corpus_bleu_score(test_captions, full_train_150_test_captions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0184f448d6494873b5885b7cafa76c11f0e318a0940098d9222ca8536d4b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
